{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"XGBoost","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyN7RearT7RICQDV2ikOIHEu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# XGBoost hyperparameters \n","\n","**XGBoost Hyperparameters have been divided into 4 categories.** They are as follows -\n","\n","1. General parameters\n","2. Booster parameters\n","3. Learning task parameters\n","4. Command line parameters"],"metadata":{"id":"iKwP96fMBSNG"}},{"cell_type":"markdown","source":["## **1-General Parameters**\n","\n","These parameters guide the overall functioning of the XGBoost model.\n","\n","In this section, we will discuss three hyperparameters - booster, verbosity and nthread.\n","\n","**1.1 booster (default =gbtree)**\n","\n","booster parameter helps us to choose which booster to use.\n","It helps us to select the type of model to run at each iteration.\n","It has 3 options - gbtree, gblinear or dart.\n","\n","gbtree and dart - use tree-based models, while\n","gblinear uses linear models.\n","\n","**1.2 verbosity**\n","\n","Verbosity of printing messages.\n","Valid values are 0 (silent), 1 (warning), 2 (info), 3 (debug).\n","\n","**1.3 nthread(default = maximum number of threads available if not set)**\n","\n","This is number of parallel threads used to run XGBoost.This is used for parallel processing and number of cores in the system should be entered.If you wish to run on all cores, value should not be entered and algorithm will detect automatically.\n"],"metadata":{"id":"CzpqqKYrBjpZ"}},{"cell_type":"markdown","source":["## **2-Booster Parameters**\n","\n","We have 2 types of boosters - tree booster and linear booster.\n","We will limit our discussion to tree booster because it always outperforms the linear booster and thus the later is rarely used.\n","\n","**2.1 eta(default=0.3, alias: learning_rate)**\n","\n","* It is analogous to learning rate in GBM.\n","* It is the step size shrinkage used in update to prevent overfitting.\n","* After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative.\n","* It makes the model more robust by shrinking the weights on each step.\n","\n","**2.2 gamma(default=0, alias: min_split_loss)**\n","\n","* A node is split only when the resulting split gives a positive reduction in the loss function.\n","* Gamma specifies the minimum loss reduction required to make a split.\n","* It makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n","* The larger gamma is, the more conservative the algorithm will be.\n","\n","**2.3 max_depth(default=6)**\n","\n","* The maximum depth of a tree, same as GBM.\n","* It is used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\n","* Increasing this value will make the model more complex and more likely to overfit.\n","* The value 0 is only accepted in lossguided growing policy when tree_method is set as hist and it indicates no limit on depth.\n","* We should be careful when setting large value of max_depth because XGBoost aggressively consumes memory when training a deep tree.\n","\n","**2.4 min_child_weight(default=1)**\n","\n","* It defines the minimum sum of weights of all observations required in a child.\n","* This is similar to min_child_leaf in GBM but not exactly. This refers to min “sum of weights” of observations while GBM has min “number of observations”.\n","* It is used to control over-fitting.\n","* Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n","* Too high values can lead to under-fitting.Hence, it should be tuned using CV.The larger min_child_weight is, the more conservative the algorithm will be.\n","\n","**2.5 max_delta_step(default=0)**\n","\n","* In maximum delta step we allow each tree’s weight estimation to be.\n","* If the value is set to 0, it means there is no constraint.\n","* If it is set to a positive value, it can help making the update step more conservative.\n","* Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced.\n","\n","**2.6 subsample(default=1)**\n","\n","* It denotes the fraction of observations to be randomly samples for each tree.\n","* Subsample ratio of the training instances.\n","* Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. - This will prevent overfitting.\n","* Subsampling will occur once in every boosting iteration.\n","* Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting.\n","Typical values: 0.5-1\n","\n","**2.7 colsample_bytree, colsample_bylevel, colsample_bynode**\n","\n","* colsample_bytree, colsample_bylevel, colsample_bynode [default=1]\n","All colsample_by parameters have a range of (0, 1], the default value of 1, and specify the fraction of columns to be subsampled.\n","\n","* colsample_bytree is the subsample ratio of columns when constructing each tree. Subsampling occurs once for every tree constructed.\n","colsample_bylevel is the subsample ratio of columns for each level. Subsampling occurs once for every new depth level reached in a tree. Columns are subsampled from the set of columns chosen for the current tree.\n","\n","* colsample_bynode is the subsample ratio of columns for each node (split). Subsampling occurs once every time a new split is evaluated. Columns are subsampled from the set of columns chosen for the current level.\n","\n","* colsample_by* parameters work cumulatively. For instance, the combination {'colsample_bytree':0.5, 'colsample_bylevel':0.5, 'colsample_bynode':0.5} with 64 features will leave 8 features to choose from at each split.\n","\n","**2.8 lambda(default=1, alias: reg_lambda)**\n","\n","* L2 regularization term on weights (analogous to Ridge regression).\n","* This is used to handle the regularization part of XGBoost.\n","* Increasing this value will make model more conservative.\n","\n","**2.9 alpha(default=0, alias: reg_alpha)**\n","\n","* L1 regularization term on weights (analogous to Lasso regression).\n","* It can be used in case of very high dimensionality so that the algorithm runs faster when implemented.\n","* Increasing this value will make model more conservative.\n","\n","**2.10 tree_method string(default= auto)**\n","\n","* XGBoost supports approx, hist and gpu_hist for distributed training. Experimental support for external memory is available for approx and gpu_hist.\n","* Choices: auto, exact, approx, hist, gpu_hist\n","* auto: Use heuristic to choose the fastest method.\n","* For small to medium dataset, exact greedy (exact) will be used.\n","* For very large dataset, approximate algorithm (approx) will be chosen.\n","* Because old behavior is always use exact greedy in single machine, user will get a message when approximate algorithm is chosen to notify this choice.\n","* exact: Exact greedy algorithm.\n","* approx: Approximate greedy algorithm using quantile sketch and gradient histogram.\n","* hist: Fast histogram optimized approximate greedy algorithm. It uses some performance improvements such as bins caching.\n","* gpu_hist: GPU implementation of hist algorithm.\n","\n","**2.11 scale_pos_weight(default=1)**\n","\n","* It controls the balance of positive and negative weights,\n","* It is useful for imbalanced classes.\n","* A value greater than 0 should be used in case of high class imbalance as it helps in faster convergence.\n","* A typical value to consider: sum(negative instances) / sum(positive instances)"],"metadata":{"id":"Ybat5iAVB_tr"}},{"cell_type":"markdown","source":["## 3-Learning Task Parameters \n","\n","These parameters are used to define the optimization objective the metric to be calculated at each step.\n","They are used to specify the learning task and the corresponding learning objective. The objective options are below:\n","\n","**3.1 objective(default=reg:squarederror)**\n","\n","It defines the loss function to be minimized. Most commonly used values are given below -\n","\n","* ***reg:squarederror*** : regression with squared loss.\n","* ***reg:squaredlogerror***: regression with squared log loss 1/2*(log(pred+1)−log(label+1)) - All input labels are required to be greater than -1.\n","* ***reg:logistic*** : logistic regression\n","* ***binary:logistic*** : logistic regression for binary classification, output probability\n","* ***binary:logitraw***: logistic regression for binary classification, output score before logistic transformation\n","* ***binary:hinge*** : hinge loss for binary classification. This makes predictions of 0 or 1, rather than producing probabilities.\n","* ***multi:softmax*** : set XGBoost to do multiclass classification using the softmax objective, you also need to set num_class(number of classes)\n","* ***multi:softprob*** : same as softmax, but output a vector of ndata nclass, which can be further reshaped to ndata nclass matrix. The result contains predicted probability of each data point belonging to each class.\n","\n","**3.2 eval_metric(default according to objective)**\n","\n","The metric to be used for validation data.The default values are rmse for regression, error for classification and mean average precision for ranking.\n","We can add multiple evaluation metrics.Python users must pass the metrices as list of parameters pairs instead of map.The most common values are given below -\n","\n","* ***rmse*** : root mean square error\n","* ***mae*** : mean absolute error\n","* ***logloss*** : negative log-likelihood \n","* ***error*** : Binary classification error rate (0.5 threshold). It is calculated as #(wrong cases)/#(all cases). For the predictions, the evaluation will regard the instances with prediction value larger than 0.5 as positive instances, and the others as negative instances.\n","* ***merror*** : Multiclass classification error rate. It is calculated as #(wrong cases)/#(all cases).\n","* ***mlogloss*** : Multiclass logloss\n","* ***auc: Area under*** the curve\n","* ***aucpr*** : Area under the PR curve\n","\n","\n","**3.3 seed(default=0)**\n","\n","* The random number seed.\n","* This parameter is ignored in R package, use set.seed() instead.\n","* It can be used for generating reproducible results and also for parameter tuning."],"metadata":{"id":"4QuY1z_vCycK"}}]}