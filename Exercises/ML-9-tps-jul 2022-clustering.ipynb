{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-19T09:47:06.126351Z","iopub.execute_input":"2022-07-19T09:47:06.126840Z","iopub.status.idle":"2022-07-19T09:47:06.157585Z","shell.execute_reply.started":"2022-07-19T09:47:06.126737Z","shell.execute_reply":"2022-07-19T09:47:06.156253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Clustering**\n\n**Clustering is the task of dividing the population or data points into a number of groups such that data points in the same groups are more similar to other data points in the same group than those in other groups. In simple words, the aim is to segregate groups with similar traits and assign them into clusters.**\n\n#### **Types of clustering algorithms**\nSince the task of clustering is subjective, the means that can be used for achieving this goal are plenty.\n\n1. **Connectivity models:** As the name suggests, these models are based on the notion that the data points closer in data space exhibit more similarity to each other than the data points lying farther away. These models can follow two approaches. In the first approach, they start with classifying all data points into separate clusters & then aggregating them as the distance decreases. In the second approach, all data points are classified as a single cluster and then partitioned as the distance increases. Also, the choice of distance function is subjective. These models are very easy to interpret but lacks scalability for handling big datasets. Examples of these models are hierarchical clustering algorithm and its variants.\n\n2. **Centroid models:** These are iterative clustering algorithms in which the notion of similarity is derived by the closeness of a data point to the centroid of the clusters. K-Means clustering algorithm is a popular algorithm that falls into this category. In these models, the no. of clusters required at the end have to be mentioned beforehand, which makes it important to have prior knowledge of the dataset. These models run iteratively to find the local optima.\n\n3. **Distribution models:** These clustering models are based on the notion of how probable is it that all data points in the cluster belong to the same distribution (For example: Normal, Gaussian). These models often suffer from overfitting. A popular example of these models is Expectation-maximization algorithm which uses multivariate normal distributions.\n\n4. **Density Models:** These models search the data space for areas of varied density of data points in the data space. It isolates various different density regions and assign the data points within these regions in the same cluster. Popular examples of density models are DBSCAN and OPTICS.\n\n***I will be taking you through two of the most popular clustering algorithms in detail – K Means clustering and Hierarchical clustering.***","metadata":{}},{"cell_type":"markdown","source":"## **1. K-Means Clustering**\n\nk-Means is an unsupervised algorithm used for clustering. By unsupervised we mean that we don’t have any labeled data upfront to train the model. Hence the algorithm just relies on the dynamics of the independent features to make inferences on unseen data.\n\n\n#### **Working of K-Means Algorithm**\n1. First, we need to provide the number of clusters, K, that need to be generated by this algorithm.\n2. Next, choose K data points at random and assign each to a cluster. Briefly, categorize the data based on the number of data points.\n3. The cluster centroids will now be computed.\n4. Iterate the steps below until we find the ideal centroid, which is the assigning of data points to clusters that do not vary.\n5. The sum of squared distances between data points and centroids would be calculated first.\n6. At this point, we need to allocate each data point to the cluster that is closest to the others (centroid).\n7. Finally, compute the centroids for the clusters by averaging all of the cluster’s data points.\n8. K-means implements the ***Expectation-Maximization*** strategy to solve the problem. The Expectation-step is used to assign data points to the nearest cluster, and the Maximization-step is used to compute the centroid of each cluster.\n\n* ***It is suggested to normalize the data while dealing with clustering algorithms such as K-Means since such algorithms employ distance-based measurement to identify the similarity between data points.***\n* ***Because of the iterative nature of K-Means and the random initialization of centroids, K-Means may become stuck in a local optimum and fail to converge to the global optimum. As a result, it is advised to employ distinct centroids’ initializations.***\n\n![kmeans](https://miro.medium.com/max/1400/1*b2sO2f--yfZiJazc5rYSpg.gif)\n\n### ***Advantages***\n1. Fast, robust and easier to understand.\n2. Relatively efficient: O(tknd), where n is # objects, k is # clusters, d is # dimension of each object, and t  is # iterations. Normally, k, t, d << n.\n3. Gives best result when data set are distinct or well separated from each other.\n\n### ***Disadvantages***\n1. The learning algorithm requires apriori specification of the number of  cluster centers.\n2. The use of  Exclusive Assignment - If  there are two highly overlapping data then k-means will not be able to resolve that there are two clusters.\n3. The learning algorithm is not invariant to non-linear transformations i.e. with different representation of data we get\n4. Euclidean distance measures can unequally weight underlying factors.\n5. The learning algorithm provides the local optima of the squared error function. \n6. Randomly choosing of the cluster center cannot lead us to the fruitful result. Pl. refer Fig.\n7. Applicable only when mean is defined i.e. fails for categorical data.\n8. Unable to handle noisy data and outliers.\n9. Algorithm fails for non-linear data set.\n","metadata":{}},{"cell_type":"markdown","source":"## **2. Hierarchical Clustering**\n\nAlso called Hierarchical cluster analysis or HCA is an unsupervised clustering algorithm which involves creating clusters that have predominant ordering from top to bottom.\n\nFor e.g: All files and folders on our hard disk are organized in a hierarchy.\n\nThe algorithm groups similar objects into groups called clusters. The endpoint is a set of clusters or groups, where each cluster is distinct from each other cluster, and the objects within each cluster are broadly similar to each other.\n\nThis clustering technique is divided into two types:\n\n* Agglomerative Hierarchical Clustering\n* Divisive Hierarchical Clustering\n\n### **2.1. Agglomerative Hierarchy**\n \nThe Agglomerative Hierarchical Clustering is the most common type of hierarchical clustering used to group objects in clusters based on their similarity. It’s also known as AGNES (Agglomerative Nesting). **It's a “bottom-up” approach**: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.\n\n![agglomerative](https://dashee87.github.io/images/hierarch.gif)\n\n### **2.2. Divisive Hierarchy**\n\nAlso known as **top-down approach**. This algorithm also does not require to prespecify the number of clusters. Top-down clustering requires a method for splitting a cluster that contains the whole data and proceeds by splitting clusters recursively until individual data have been splitted into singleton cluster.\n\n![divisive](https://www.researchgate.net/profile/Elias-Giacoumidis/publication/321399805/figure/fig2/AS:631627432603703@1527603123716/Conceptual-dendrogram-for-agglomerative-and-divisive-Hierarchical-based-clustering-19.png)","metadata":{}},{"cell_type":"markdown","source":"## **3. Density Based(DBSCAN)**\n\n***Density-based spatial clustering of applications with noise (DBSCAN)***\n\nIt can discover clusters of different shapes and sizes from a large amount of data, which is containing noise and outliers.\n\nThe DBSCAN algorithm uses two parameters:\n\n* **minPts:** The minimum number of points (a threshold) clustered together for a region to be considered dense.\n* **eps (ε):** A distance measure that will be used to locate the points in the neighborhood of any point.\n\nThere are three types of points after the DBSCAN clustering is complete:\n* **Core —** This is a point that has at least m points within distance n from itself.\n* **Border —** This is a point that has at least one Core point at a distance n.\n* **Noise —** This is a point that is neither a Core nor a Border. And it has less than m points within distance n from itself.\n\n![dbscan](https://dashee87.github.io/images/DBSCAN_tutorial.gif)\n\n**Why do we need a Density-Based clustering algorithm like DBSCAN when we already have K-means clustering?**\n\nK-Means clustering may cluster loosely related observations together. Every observation becomes a part of some cluster eventually, even if the observations are scattered far away in the vector space. Since clusters depend on the mean value of cluster elements, each data point plays a role in forming the clusters. A slight change in data points might affect the clustering outcome. This problem is greatly reduced in DBSCAN due to the way clusters are formed. This is usually not a big problem unless we come across some odd shape data.\n\nAnother challenge with k-means is that you need to specify the number of clusters (“k”) in order to use it. Much of the time, we won’t know what a reasonable k value is a priori.\n\nWhat’s nice about DBSCAN is that you don’t have to specify the number of clusters to use it. All you need is a function to calculate the distance between values and some guidance for what amount of distance is considered “close”. DBSCAN also produces more reasonable results than k-means across a variety of different distributions.\n\n#### **Advantages of DBSCAN**\n\n* Is great at separating clusters of high density versus clusters of low density within a given dataset.\n* Is great with handling outliers within the dataset.\n\n#### **Disadvantages of DBSCAN**\n\n* While DBSCAN is great at separating high density clusters from low density clusters, DBSCAN struggles with clusters of similar density.\n* Struggles with high dimensionality data.","metadata":{}},{"cell_type":"markdown","source":"## **4-Gaussian Mixture Model**\n\nGaussian Mixture Models (GMMs) assume that there are a certain number of Gaussian distributions, and each of these distributions represent a cluster. Hence, a Gaussian Mixture Model tends to group the data points belonging to a single distribution together.\n\n***Gaussian Mixture Models are probabilistic models and use the soft clustering approach for distributing the points in different clusters.***\n\nThere are the two basic steps of the EM algorithm, namely E Step or Expectation Step or Estimation Step and M Step or Maximization Step.\n \n\n#### ***Estimation step***\n\n* initialize mu_k   , Sigma_k   and pi_k   by some random values, or by K means clustering results or by hierarchical clustering results.\n* Then for those given parameter values, estimate the value of the latent variables (i.e gamma_k   )\n\n#### ***Maximization Step***\n\n* Update the value of the parameters( i.e. mu_k   , Sigma_k and pi_k) calculated using ML method.\n\n![gaussian](https://www.mlpack.org/gsocblog/images/5_clusters_QGMM.gif)\n\n***Gaussian Mixture Models are a very powerful tool and are widely used in diverse tasks that involve data clustering.***","metadata":{}},{"cell_type":"markdown","source":"# **Import**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style='darkgrid', font_scale=1.4)","metadata":{"execution":{"iopub.status.busy":"2022-07-19T09:47:06.271460Z","iopub.execute_input":"2022-07-19T09:47:06.272253Z","iopub.status.idle":"2022-07-19T09:47:06.883378Z","shell.execute_reply.started":"2022-07-19T09:47:06.272205Z","shell.execute_reply":"2022-07-19T09:47:06.882361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data**","metadata":{}},{"cell_type":"code","source":"data=pd.read_csv('/kaggle/input/tabular-playground-series-jul-2022/data.csv', index_col='id')\nprint(f'Shape : {data.shape}')\ndata.head(10)\n      ","metadata":{"execution":{"iopub.status.busy":"2022-07-19T09:47:06.885336Z","iopub.execute_input":"2022-07-19T09:47:06.885682Z","iopub.status.idle":"2022-07-19T09:47:08.370675Z","shell.execute_reply.started":"2022-07-19T09:47:06.885649Z","shell.execute_reply":"2022-07-19T09:47:08.369482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observe, data is already standardized & encoded**","metadata":{}},{"cell_type":"markdown","source":"# **Information**","metadata":{}},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2022-07-19T09:47:08.372157Z","iopub.execute_input":"2022-07-19T09:47:08.372545Z","iopub.status.idle":"2022-07-19T09:47:08.403456Z","shell.execute_reply.started":"2022-07-19T09:47:08.372511Z","shell.execute_reply":"2022-07-19T09:47:08.401882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"execution":{"iopub.status.busy":"2022-07-19T09:47:08.406463Z","iopub.execute_input":"2022-07-19T09:47:08.407073Z","iopub.status.idle":"2022-07-19T09:47:08.625779Z","shell.execute_reply.started":"2022-07-19T09:47:08.407030Z","shell.execute_reply":"2022-07-19T09:47:08.624547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**All column counts are 98000,means NO NULL values**","metadata":{}},{"cell_type":"markdown","source":"# **Exploratory Data Analysis**","metadata":{}},{"cell_type":"markdown","source":"## **Discreet Features**\n\n* There are 7 discrete features: f_07 to f_13.\n* Values are non-negative.\n* Distributions are all similar.","metadata":{}},{"cell_type":"code","source":"# Figure with subplots\nfig=plt.figure(figsize=(15,14))\n\nfor i in range(7):\n    # New subplot\n    plt.subplot(4,2,i+1)\n    feat_num=i+7\n    sns.countplot(x=data.iloc[:,feat_num])\n    \n    # Aesthetics\n    plt.title(f'Feature: 0{feat_num}')\n    plt.xlim([-1,44])      # same scale for all plots\n    plt.ylim([0,11000])   # same scale for all plots\n    plt.xticks(np.arange(0,44,2))\n    plt.xlabel('')\n    \n# Overall aesthetics\nfig.suptitle('Discrete feature distributions',  size=20)\nfig.tight_layout()  # Improves appearance a bit\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-19T09:47:08.627390Z","iopub.execute_input":"2022-07-19T09:47:08.627837Z","iopub.status.idle":"2022-07-19T09:47:10.765273Z","shell.execute_reply.started":"2022-07-19T09:47:08.627786Z","shell.execute_reply":"2022-07-19T09:47:10.764455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Continous Features**\n\n* There are 22 continuous features: f_00 to f_06 and f_14 to f_28\n* Distributions are all Normal, usually with mean 0 and standard deviation 1.\n* Values typically lie between -5 and +5.","metadata":{}},{"cell_type":"code","source":"# Continuous features\ncont_feats=[f'f_0{i}' for i in range(7)]\ncont_feats=cont_feats + [f'f_{i}' for i in range(14,29)]\n\n# Figure with subplots\nfig=plt.figure(figsize=(15,14))\n\nfor i, f in enumerate(cont_feats):\n    # New subplot\n    plt.subplot(6,4,i+1)\n    sns.histplot(x=data[f])\n    \n    # Aesthetics\n    plt.title(f'Feature: {f}')\n    plt.xlabel('')\n    \n# Overall aesthetics\nfig.suptitle('Continuous feature distributions',  size=20)\nfig.tight_layout()  # Improves appearance a bit\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-19T09:47:10.766344Z","iopub.execute_input":"2022-07-19T09:47:10.766868Z","iopub.status.idle":"2022-07-19T09:47:19.989833Z","shell.execute_reply.started":"2022-07-19T09:47:10.766838Z","shell.execute_reply":"2022-07-19T09:47:19.989006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model**\n\nA Bayesian Gaussian Mixture Model (BGMM) is very similar to a GMM. It uses the same assumptions on the data and the same approach to finding the clusters. ***The only difference is in the learning algorithm. Instead of Maximum Likelihood Estimation, BGMMs use Variational Bayesian Estimation.In contrast to traditional frameworks, the Bayesian approach views parameters as random variables rather than fixed unknown quantities. It estimates the distribution of these parameters by sampling from the posterior distribution using methods like Markov Chain Monte Carlo (MCMC).***","metadata":{}},{"cell_type":"code","source":"from sklearn.mixture import BayesianGaussianMixture\nbgmm = BayesianGaussianMixture(n_components=7)","metadata":{"execution":{"iopub.status.busy":"2022-07-19T09:47:19.990951Z","iopub.execute_input":"2022-07-19T09:47:19.991670Z","iopub.status.idle":"2022-07-19T09:47:20.348009Z","shell.execute_reply.started":"2022-07-19T09:47:19.991639Z","shell.execute_reply":"2022-07-19T09:47:20.346919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Train**","metadata":{}},{"cell_type":"code","source":"bgmm.fit(data)","metadata":{"execution":{"iopub.status.busy":"2022-07-19T09:47:20.349283Z","iopub.execute_input":"2022-07-19T09:47:20.349721Z","iopub.status.idle":"2022-07-19T09:48:24.894855Z","shell.execute_reply.started":"2022-07-19T09:47:20.349688Z","shell.execute_reply":"2022-07-19T09:48:24.893589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Predict**","metadata":{}},{"cell_type":"code","source":"pred=bgmm.predict(data)\npred","metadata":{"execution":{"iopub.status.busy":"2022-07-19T09:48:24.900668Z","iopub.execute_input":"2022-07-19T09:48:24.901975Z","iopub.status.idle":"2022-07-19T09:48:25.238743Z","shell.execute_reply.started":"2022-07-19T09:48:24.901916Z","shell.execute_reply":"2022-07-19T09:48:25.237429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Visualize Predictions**","metadata":{}},{"cell_type":"code","source":"# Countplot\nplt.figure(figsize=(10,4))\nsns.countplot(x=pred)\nplt.title('Predicted clusters')","metadata":{"execution":{"iopub.status.busy":"2022-07-19T09:48:25.243007Z","iopub.execute_input":"2022-07-19T09:48:25.244011Z","iopub.status.idle":"2022-07-19T09:48:25.475842Z","shell.execute_reply.started":"2022-07-19T09:48:25.243958Z","shell.execute_reply":"2022-07-19T09:48:25.474681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bgmm.get_params(deep=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-19T09:48:25.477190Z","iopub.execute_input":"2022-07-19T09:48:25.477523Z","iopub.status.idle":"2022-07-19T09:48:25.485780Z","shell.execute_reply.started":"2022-07-19T09:48:25.477472Z","shell.execute_reply":"2022-07-19T09:48:25.484580Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Suggestions:-**\n\n* Kaggle - https://www.kaggle.com/pythonkumar\n* GitHub - https://github.com/KumarPython​\n* Twitter - https://twitter.com/KumarPython\n* LinkedIn - https://www.linkedin.com/in/kumarpython/","metadata":{}},{"cell_type":"markdown","source":"# **Submission**","metadata":{}},{"cell_type":"code","source":"data.index","metadata":{"execution":{"iopub.status.busy":"2022-07-19T09:49:17.908387Z","iopub.execute_input":"2022-07-19T09:49:17.908864Z","iopub.status.idle":"2022-07-19T09:49:17.917049Z","shell.execute_reply.started":"2022-07-19T09:49:17.908824Z","shell.execute_reply":"2022-07-19T09:49:17.915995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission=pd.DataFrame({\n    'Id':data.index,\n    'Predicted':pred\n})\n\nsubmission\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-07-19T09:51:41.956110Z","iopub.execute_input":"2022-07-19T09:51:41.956595Z","iopub.status.idle":"2022-07-19T09:51:42.060982Z","shell.execute_reply.started":"2022-07-19T09:51:41.956559Z","shell.execute_reply":"2022-07-19T09:51:42.059672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **(Extra) - Dendogram**\n\n### **Dendogram can only be constructed for Hierarchical Models***","metadata":{}},{"cell_type":"code","source":"# Verticle Dendogram for Agglomerative Clustering\nfrom sklearn.cluster import AgglomerativeClustering\n\ncluster = AgglomerativeClustering(n_clusters=7, affinity='euclidean', linkage='ward')\ncluster.fit_predict(data)\n\nimport scipy.cluster.hierarchy as shc\n\nplt.figure(figsize=(10, 7))\nplt.title(\"Dendograms\")\ndend = shc.dendrogram(shc.linkage(data, method='ward'))","metadata":{"execution":{"iopub.status.busy":"2022-07-19T09:57:28.956710Z","iopub.execute_input":"2022-07-19T09:57:28.957157Z"},"trusted":true},"execution_count":null,"outputs":[]}]}